{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "validation = pd.read_csv(\"../data/valid.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label_2'].fillna(train['label_2'].mean(), inplace=True)\n",
    "validation['label_2'].fillna(validation['label_2'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label_2'] = train['label_2'].astype(int)\n",
    "validation['label_2'] = validation['label_2'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_2\n",
       "26    4762\n",
       "27    3326\n",
       "25    2849\n",
       "23    2842\n",
       "31    2385\n",
       "24    1906\n",
       "28    1899\n",
       "30    1894\n",
       "22    1432\n",
       "29    1424\n",
       "33     945\n",
       "36     481\n",
       "35     480\n",
       "34     478\n",
       "32     476\n",
       "41     474\n",
       "61     467\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"label_2\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def remove_outliers(df, threshold):\n",
    "    # Copy the DataFrame\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Remove the columns to exclude from the copy\n",
    "    df_copy = df_copy.drop([\"label_1\", \"label_2\", \"label_3\", \"label_4\"], axis=1)\n",
    "    \n",
    "    # Calculate Z-scores only on remaining columns\n",
    "    z_scores = stats.zscore(df_copy)\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    \n",
    "    # Identify outliers\n",
    "    filtered_entries = (abs_z_scores < threshold).all(axis=1)\n",
    "    \n",
    "    # Apply the mask to the original DataFrame\n",
    "    new_df = df[filtered_entries]\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# Usage:\n",
    "# exclude_cols is a list of column names to exclude\n",
    "# train_df = remove_outliers(train_df, 3, exclude_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_dataset(df, label):\n",
    "    X = df.drop([\"label_1\", \"label_2\", \"label_3\", \"label_4\"], axis=1)\n",
    "    y = df[label]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def train_model(X_train, y_train, C=1.0, kernel='rbf', degree=3, gamma='scale'):\n",
    "    model = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    print(f\"Training score: {model.score(X_train, y_train)}\")\n",
    "    print(f\"Testing score: {model.score(X_test, y_test)}\")\n",
    "\n",
    "def evaluate_model_detailed(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.124623</td>\n",
       "      <td>0.196628</td>\n",
       "      <td>0.257004</td>\n",
       "      <td>-0.156045</td>\n",
       "      <td>-0.054916</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>-0.035149</td>\n",
       "      <td>-0.092019</td>\n",
       "      <td>-0.196302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221466</td>\n",
       "      <td>0.140292</td>\n",
       "      <td>0.123622</td>\n",
       "      <td>-0.175572</td>\n",
       "      <td>-0.107030</td>\n",
       "      <td>-0.087621</td>\n",
       "      <td>-0.026501</td>\n",
       "      <td>0.139337</td>\n",
       "      <td>-0.083030</td>\n",
       "      <td>0.059507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.109655</td>\n",
       "      <td>0.170158</td>\n",
       "      <td>0.227644</td>\n",
       "      <td>-0.127088</td>\n",
       "      <td>-0.044476</td>\n",
       "      <td>-0.046852</td>\n",
       "      <td>-0.090026</td>\n",
       "      <td>-0.061321</td>\n",
       "      <td>-0.227288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204930</td>\n",
       "      <td>0.110203</td>\n",
       "      <td>0.085665</td>\n",
       "      <td>-0.286787</td>\n",
       "      <td>-0.113195</td>\n",
       "      <td>-0.057312</td>\n",
       "      <td>-0.055680</td>\n",
       "      <td>0.143939</td>\n",
       "      <td>-0.045760</td>\n",
       "      <td>0.106113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.014854</td>\n",
       "      <td>0.030051</td>\n",
       "      <td>0.115092</td>\n",
       "      <td>-0.017179</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>-0.011692</td>\n",
       "      <td>-0.078855</td>\n",
       "      <td>-0.042991</td>\n",
       "      <td>-0.096283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032937</td>\n",
       "      <td>0.075821</td>\n",
       "      <td>0.030987</td>\n",
       "      <td>-0.149850</td>\n",
       "      <td>-0.003155</td>\n",
       "      <td>-0.010207</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>-0.017069</td>\n",
       "      <td>0.048123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.196893</td>\n",
       "      <td>0.113314</td>\n",
       "      <td>0.352175</td>\n",
       "      <td>-0.108499</td>\n",
       "      <td>-0.064472</td>\n",
       "      <td>-0.073239</td>\n",
       "      <td>-0.086402</td>\n",
       "      <td>0.008671</td>\n",
       "      <td>-0.342217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255167</td>\n",
       "      <td>0.096579</td>\n",
       "      <td>0.069413</td>\n",
       "      <td>-0.215386</td>\n",
       "      <td>-0.075168</td>\n",
       "      <td>-0.035071</td>\n",
       "      <td>-0.023375</td>\n",
       "      <td>0.067768</td>\n",
       "      <td>-0.181530</td>\n",
       "      <td>0.174444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.033004</td>\n",
       "      <td>0.013373</td>\n",
       "      <td>0.124001</td>\n",
       "      <td>-0.016143</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>-0.055789</td>\n",
       "      <td>-0.036282</td>\n",
       "      <td>-0.059422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035814</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>0.027321</td>\n",
       "      <td>-0.116009</td>\n",
       "      <td>0.010096</td>\n",
       "      <td>-0.042293</td>\n",
       "      <td>0.005347</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>-0.007731</td>\n",
       "      <td>0.058799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   1   0.124623   0.196628   0.257004  -0.156045  -0.054916   0.006071   \n",
       "1   2   0.109655   0.170158   0.227644  -0.127088  -0.044476  -0.046852   \n",
       "2   3   0.014854   0.030051   0.115092  -0.017179   0.002720  -0.011692   \n",
       "3   4   0.196893   0.113314   0.352175  -0.108499  -0.064472  -0.073239   \n",
       "4   5   0.033004   0.013373   0.124001  -0.016143   0.010120   0.010635   \n",
       "\n",
       "   feature_7  feature_8  feature_9  ...  feature_759  feature_760  \\\n",
       "0  -0.035149  -0.092019  -0.196302  ...    -0.221466     0.140292   \n",
       "1  -0.090026  -0.061321  -0.227288  ...    -0.204930     0.110203   \n",
       "2  -0.078855  -0.042991  -0.096283  ...    -0.032937     0.075821   \n",
       "3  -0.086402   0.008671  -0.342217  ...    -0.255167     0.096579   \n",
       "4  -0.055789  -0.036282  -0.059422  ...    -0.035814     0.093764   \n",
       "\n",
       "   feature_761  feature_762  feature_763  feature_764  feature_765  \\\n",
       "0     0.123622    -0.175572    -0.107030    -0.087621    -0.026501   \n",
       "1     0.085665    -0.286787    -0.113195    -0.057312    -0.055680   \n",
       "2     0.030987    -0.149850    -0.003155    -0.010207    -0.001427   \n",
       "3     0.069413    -0.215386    -0.075168    -0.035071    -0.023375   \n",
       "4     0.027321    -0.116009     0.010096    -0.042293     0.005347   \n",
       "\n",
       "   feature_766  feature_767  feature_768  \n",
       "0     0.139337    -0.083030     0.059507  \n",
       "1     0.143939    -0.045760     0.106113  \n",
       "2     0.000934    -0.017069     0.048123  \n",
       "3     0.067768    -0.181530     0.174444  \n",
       "4     0.007722    -0.007731     0.058799  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28520, 772) (750, 772)\n",
      "(28456, 772) (748, 772)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, validation.shape)\n",
    "\n",
    "# Assuming train_df is your training DataFrame and valid_df is your validation DataFrame\n",
    "train = remove_outliers(train, 8)\n",
    "validation = remove_outliers(validation, 8)\n",
    "\n",
    "print(train.shape, validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = separate_dataset(train, \"label_2\")\n",
    "X_valid, y_valid = separate_dataset(validation, \"label_2\")\n",
    "# X_test, y_test = separate_dataset(test, \"label_2\")\n",
    "test_md = test.drop(\"ID\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_features(X_train, X_test, X_val):\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Save the column names\n",
    "    columns = X_train.columns\n",
    "    \n",
    "    # Fit the scaler to the training data and transform\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Transform the test and validation data\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Convert the scaled features into DataFrames\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=columns)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=columns)\n",
    "    X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=columns)\n",
    "    \n",
    "    return X_train_scaled_df, X_test_scaled_df, X_val_scaled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, X_valid_scaled = scale_features(X_train, test_md, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 768\n",
      "Reduced number of features: 221\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA that will retain 99% of variance\n",
    "pca = PCA(n_components=0.97, whiten=True)\n",
    "\n",
    "# Fit PCA on the training set\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Apply the transformation to the validation and test sets\n",
    "X_valid_pca = pca.transform(X_valid_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print('Original number of features:', X_train.shape[1])\n",
    "print('Reduced number of features:', X_train_pca.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train_pca)\n",
    "X_test_df = pd.DataFrame(X_test_pca)\n",
    "X_valid_df = pd.DataFrame(X_valid_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, X_valid_scaled = scale_features(X_train_df, X_test_df, X_valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(X_train_scaled, y_train, degree=3, C=1.0, gamma=\"auto\", kernel=\"rbf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.9470410458251335\n",
      "Testing score: 0.8449197860962567\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          22       0.93      0.75      0.83        36\n",
      "          23       0.89      0.79      0.84        71\n",
      "          24       0.98      0.87      0.92        46\n",
      "          25       0.86      0.80      0.83        79\n",
      "          26       0.72      0.96      0.82       115\n",
      "          27       0.66      0.91      0.77        80\n",
      "          28       0.95      0.68      0.79        59\n",
      "          29       1.00      0.84      0.92        45\n",
      "          30       0.98      0.85      0.91        48\n",
      "          31       0.84      0.89      0.87        65\n",
      "          32       1.00      0.82      0.90        11\n",
      "          33       0.96      0.87      0.91        30\n",
      "          34       0.91      0.91      0.91        11\n",
      "          35       1.00      0.82      0.90        11\n",
      "          36       1.00      1.00      1.00         8\n",
      "          41       0.91      0.71      0.80        14\n",
      "          61       1.00      0.74      0.85        19\n",
      "\n",
      "    accuracy                           0.84       748\n",
      "   macro avg       0.92      0.84      0.87       748\n",
      "weighted avg       0.87      0.84      0.85       748\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 27   0   0   1   5   1   0   0   0   2   0   0   0   0   0   0   0]\n",
      " [  1  56   1   2   6   4   0   0   0   0   0   0   0   0   0   1   0]\n",
      " [  0   2  40   0   0   2   0   0   0   0   0   1   1   0   0   0   0]\n",
      " [  0   1   0  63  13   0   0   0   0   2   0   0   0   0   0   0   0]\n",
      " [  0   1   0   4 110   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   1   3  73   0   0   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   4  14  40   0   0   1   0   0   0   0   0   0   0]\n",
      " [  1   1   0   0   2   0   1  38   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   3   3   0   0  41   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   1   3   1   0   1  58   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   1   0   0   0   0   9   0   0   0   0   0   0]\n",
      " [  0   0   0   0   1   3   0   0   0   0   0  26   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0   0   0   0   0  10   0   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0   0   0   0   0   9   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   8   0   0]\n",
      " [  0   0   0   1   2   0   0   0   0   1   0   0   0   0   0  10   0]\n",
      " [  0   0   0   0   2   3   0   0   0   0   0   0   0   0   0   0  14]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, X_train_scaled, X_valid_scaled, y_train, y_valid)\n",
    "evaluate_model_detailed(model, X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 6\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 102\n",
      "max_resources_: 28456\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 250\n",
      "n_resources: 102\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 84\n",
      "n_resources: 306\n",
      "Fitting 3 folds for each of 84 candidates, totalling 252 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 28\n",
      "n_resources: 918\n",
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 10\n",
      "n_resources: 2754\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 4\n",
      "n_resources: 8262\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 2\n",
      "n_resources: 24786\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv  # explicitly require this experimental feature\n",
    "from sklearn.model_selection import HalvingRandomSearchCV  # noqa\n",
    "\n",
    "param_dist = {\n",
    "    'C': np.logspace(-4, 3, 8),  # expanded range\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    #'degree': [2, 3, 4, 5],  # added degree for 'poly' kernel\n",
    "    'gamma': ['scale', 'auto'] + list(np.logspace(-4, 3, 8))  # expanded range\n",
    "}\n",
    "\n",
    "# Assuming X_train and y_train are your training data and labels\n",
    "halving_random_search = HalvingRandomSearchCV(svm.SVC(random_state=42), param_dist, cv=3, verbose=10, n_jobs=-1, n_candidates=250)\n",
    "halving_random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(halving_random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 6\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 102\n",
      "max_resources_: 28456\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 250\n",
      "n_resources: 102\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 84\n",
      "n_resources: 306\n",
      "Fitting 3 folds for each of 84 candidates, totalling 252 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 28\n",
      "n_resources: 918\n",
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 10\n",
      "n_resources: 2754\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 4\n",
      "n_resources: 8262\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 2\n",
      "n_resources: 24786\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "{'kernel': 'poly', 'gamma': 1.0, 'degree': 3, 'C': 10.0}\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'C': np.logspace(-4, 3, 8),  # expanded range\n",
    "    'kernel': [ 'poly' ],\n",
    "    'degree': [2, 3, 4, 5],  # added degree for 'poly' kernel\n",
    "    'gamma': ['scale', 'auto'] + list(np.logspace(-4, 3, 8))  # expanded range\n",
    "}\n",
    "\n",
    "# Assuming X_train and y_train are your training data and labels\n",
    "halving_random_search = HalvingRandomSearchCV(svm.SVC(random_state=42), param_dist, cv=3, verbose=10, n_jobs=-1, n_candidates=250)\n",
    "halving_random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(halving_random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Academics\\Semester 7\\Machine Learning\\Assignments\\Project\\Kaggle\\New\\l2_pca.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Academics/Semester%207/Machine%20Learning/Assignments/Project/Kaggle/New/l2_pca.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_tuned \u001b[39m=\u001b[39m train_model(X_train_scaled, y_train, kernel\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrbf\u001b[39;49m\u001b[39m\"\u001b[39;49m, gamma\u001b[39m=\u001b[39;49m\u001b[39m2.0\u001b[39;49m, degree\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, C\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m)\n",
      "\u001b[1;32md:\\Academics\\Semester 7\\Machine Learning\\Assignments\\Project\\Kaggle\\New\\l2_pca.ipynb Cell 21\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Academics/Semester%207/Machine%20Learning/Assignments/Project/Kaggle/New/l2_pca.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(X_train, y_train, C\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, kernel\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrbf\u001b[39m\u001b[39m'\u001b[39m, degree\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mscale\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Academics/Semester%207/Machine%20Learning/Assignments/Project/Kaggle/New/l2_pca.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     model \u001b[39m=\u001b[39m svm\u001b[39m.\u001b[39mSVC(C\u001b[39m=\u001b[39mC, kernel\u001b[39m=\u001b[39mkernel, degree\u001b[39m=\u001b[39mdegree, gamma\u001b[39m=\u001b[39mgamma, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Academics/Semester%207/Machine%20Learning/Assignments/Project/Kaggle/New/l2_pca.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Academics/Semester%207/Machine%20Learning/Assignments/Project/Kaggle/New/l2_pca.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32md:\\Python 3.10.11\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python 3.10.11\\lib\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[LibSVM]\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[39m=\u001b[39m rnd\u001b[39m.\u001b[39mrandint(np\u001b[39m.\u001b[39miinfo(\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001b[39m=\u001b[39;49mseed)\n\u001b[0;32m    251\u001b[0m \u001b[39m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_fit_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32md:\\Python 3.10.11\\lib\\site-packages\\sklearn\\svm\\_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    315\u001b[0m libsvm\u001b[39m.\u001b[39mset_verbosity_wrap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    317\u001b[0m \u001b[39m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    319\u001b[0m (\n\u001b[0;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_,\n\u001b[0;32m    321\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_vectors_,\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_support,\n\u001b[0;32m    323\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdual_coef_,\n\u001b[0;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_,\n\u001b[0;32m    325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probA,\n\u001b[0;32m    326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probB,\n\u001b[0;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_status_,\n\u001b[0;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_iter,\n\u001b[1;32m--> 329\u001b[0m ) \u001b[39m=\u001b[39m libsvm\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    330\u001b[0m     X,\n\u001b[0;32m    331\u001b[0m     y,\n\u001b[0;32m    332\u001b[0m     svm_type\u001b[39m=\u001b[39;49msolver_type,\n\u001b[0;32m    333\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    334\u001b[0m     \u001b[39m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[0;32m    335\u001b[0m     class_weight\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m_class_weight\u001b[39;49m\u001b[39m\"\u001b[39;49m, np\u001b[39m.\u001b[39;49mempty(\u001b[39m0\u001b[39;49m)),\n\u001b[0;32m    336\u001b[0m     kernel\u001b[39m=\u001b[39;49mkernel,\n\u001b[0;32m    337\u001b[0m     C\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[0;32m    338\u001b[0m     nu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnu,\n\u001b[0;32m    339\u001b[0m     probability\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobability,\n\u001b[0;32m    340\u001b[0m     degree\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegree,\n\u001b[0;32m    341\u001b[0m     shrinking\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshrinking,\n\u001b[0;32m    342\u001b[0m     tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m    343\u001b[0m     cache_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_size,\n\u001b[0;32m    344\u001b[0m     coef0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef0,\n\u001b[0;32m    345\u001b[0m     gamma\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma,\n\u001b[0;32m    346\u001b[0m     epsilon\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon,\n\u001b[0;32m    347\u001b[0m     max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    348\u001b[0m     random_seed\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[0;32m    349\u001b[0m )\n\u001b[0;32m    351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_tuned = train_model(X_train_scaled, y_train, kernel=\"poly\", gamma=0.1, degree=2, C=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.16697054698457223\n",
      "Testing score: 0.15333333333333332\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          22       0.00      0.00      0.00        36\n",
      "          23       0.00      0.00      0.00        71\n",
      "          24       0.00      0.00      0.00        46\n",
      "          25       0.00      0.00      0.00        79\n",
      "          26       0.15      1.00      0.27       115\n",
      "          27       0.00      0.00      0.00        81\n",
      "          28       0.00      0.00      0.00        60\n",
      "          29       0.00      0.00      0.00        45\n",
      "          30       0.00      0.00      0.00        48\n",
      "          31       0.00      0.00      0.00        65\n",
      "          32       0.00      0.00      0.00        11\n",
      "          33       0.00      0.00      0.00        30\n",
      "          34       0.00      0.00      0.00        11\n",
      "          35       0.00      0.00      0.00        11\n",
      "          36       0.00      0.00      0.00         8\n",
      "          41       0.00      0.00      0.00        14\n",
      "          61       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.15       750\n",
      "   macro avg       0.01      0.06      0.02       750\n",
      "weighted avg       0.02      0.15      0.04       750\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0   0   0   0  36   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  71   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  46   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  79   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 115   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  81   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  60   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  45   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  48   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  65   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  11   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  30   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  11   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  11   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   8   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  14   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  19   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python 3.10.11\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python 3.10.11\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python 3.10.11\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_tuned, X_train_scaled, X_valid_scaled, y_train, y_valid)\n",
    "evaluate_model_detailed(model_tuned, X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned1 = train_model(X_train_scaled, y_train, kernel=\"poly\", gamma=1.0, degree=3, C=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 1.0\n",
      "Testing score: 0.8609625668449198\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          22       1.00      0.89      0.94        36\n",
      "          23       0.78      0.79      0.78        71\n",
      "          24       0.95      0.91      0.93        46\n",
      "          25       0.93      0.81      0.86        79\n",
      "          26       0.73      0.97      0.83       115\n",
      "          27       0.80      0.91      0.85        80\n",
      "          28       0.91      0.66      0.76        59\n",
      "          29       0.93      0.82      0.87        45\n",
      "          30       0.98      0.92      0.95        48\n",
      "          31       0.91      0.92      0.92        65\n",
      "          32       0.90      0.82      0.86        11\n",
      "          33       0.96      0.87      0.91        30\n",
      "          34       0.90      0.82      0.86        11\n",
      "          35       0.90      0.82      0.86        11\n",
      "          36       1.00      1.00      1.00         8\n",
      "          41       0.71      0.71      0.71        14\n",
      "          61       1.00      0.79      0.88        19\n",
      "\n",
      "    accuracy                           0.86       748\n",
      "   macro avg       0.90      0.85      0.87       748\n",
      "weighted avg       0.87      0.86      0.86       748\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 32   1   0   0   2   0   0   0   0   0   0   0   0   1   0   0   0]\n",
      " [  0  56   0   0   9   1   1   1   0   0   0   0   0   0   0   3   0]\n",
      " [  0   2  42   0   0   1   0   0   0   0   0   0   0   0   0   1   0]\n",
      " [  0   3   0  64   8   0   1   0   0   3   0   0   0   0   0   0   0]\n",
      " [  0   1   0   1 111   0   0   0   1   1   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   2  73   0   0   0   1   1   1   1   0   0   0   0]\n",
      " [  0   2   2   0   2  12  39   1   0   1   0   0   0   0   0   0   0]\n",
      " [  0   2   0   0   5   0   1  37   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   1   2   1   0  44   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   2   1   0   1   0  60   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   1   0   0   0   0   0   9   0   0   0   0   0   0]\n",
      " [  0   0   0   1   2   1   0   0   0   0   0  26   0   0   0   0   0]\n",
      " [  0   0   0   0   2   0   0   0   0   0   0   0   9   0   0   0   0]\n",
      " [  0   0   0   1   1   0   0   0   0   0   0   0   0   9   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   8   0   0]\n",
      " [  0   2   0   1   1   0   0   0   0   0   0   0   0   0   0  10   0]\n",
      " [  0   1   0   0   3   0   0   0   0   0   0   0   0   0   0   0  15]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_tuned1, X_train_scaled, X_valid_scaled, y_train, y_valid)\n",
    "evaluate_model_detailed(model_tuned1, X_valid_scaled, y_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
