{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "validation = pd.read_csv(\"../data/valid.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label_2'].fillna(train['label_2'].mean(), inplace=True)\n",
    "validation['label_2'].fillna(validation['label_2'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label_2'] = train['label_2'].astype(int)\n",
    "validation['label_2'] = validation['label_2'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def remove_outliers(df, threshold):\n",
    "    # Copy the DataFrame\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Remove the columns to exclude from the copy\n",
    "    df_copy = df_copy.drop([\"label_1\", \"label_2\", \"label_3\", \"label_4\"], axis=1)\n",
    "    \n",
    "    # Calculate Z-scores only on remaining columns\n",
    "    z_scores = stats.zscore(df_copy)\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    \n",
    "    # Identify outliers\n",
    "    filtered_entries = (abs_z_scores < threshold).all(axis=1)\n",
    "    \n",
    "    # Apply the mask to the original DataFrame\n",
    "    new_df = df[filtered_entries]\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# Usage:\n",
    "# exclude_cols is a list of column names to exclude\n",
    "# train_df = remove_outliers(train_df, 3, exclude_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_dataset(df, label):\n",
    "    X = df.drop([\"label_1\", \"label_2\", \"label_3\", \"label_4\"], axis=1)\n",
    "    y = df[label]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def train_model(X_train, y_train, C=1.0, kernel='rbf', degree=3, gamma='scale'):\n",
    "    model = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    print(f\"Training score: {model.score(X_train, y_train)}\")\n",
    "    print(f\"Testing score: {model.score(X_test, y_test)}\")\n",
    "\n",
    "def evaluate_model_detailed(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.124623</td>\n",
       "      <td>0.196628</td>\n",
       "      <td>0.257004</td>\n",
       "      <td>-0.156045</td>\n",
       "      <td>-0.054916</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>-0.035149</td>\n",
       "      <td>-0.092019</td>\n",
       "      <td>-0.196302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221466</td>\n",
       "      <td>0.140292</td>\n",
       "      <td>0.123622</td>\n",
       "      <td>-0.175572</td>\n",
       "      <td>-0.107030</td>\n",
       "      <td>-0.087621</td>\n",
       "      <td>-0.026501</td>\n",
       "      <td>0.139337</td>\n",
       "      <td>-0.083030</td>\n",
       "      <td>0.059507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.109655</td>\n",
       "      <td>0.170158</td>\n",
       "      <td>0.227644</td>\n",
       "      <td>-0.127088</td>\n",
       "      <td>-0.044476</td>\n",
       "      <td>-0.046852</td>\n",
       "      <td>-0.090026</td>\n",
       "      <td>-0.061321</td>\n",
       "      <td>-0.227288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204930</td>\n",
       "      <td>0.110203</td>\n",
       "      <td>0.085665</td>\n",
       "      <td>-0.286787</td>\n",
       "      <td>-0.113195</td>\n",
       "      <td>-0.057312</td>\n",
       "      <td>-0.055680</td>\n",
       "      <td>0.143939</td>\n",
       "      <td>-0.045760</td>\n",
       "      <td>0.106113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.014854</td>\n",
       "      <td>0.030051</td>\n",
       "      <td>0.115092</td>\n",
       "      <td>-0.017179</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>-0.011692</td>\n",
       "      <td>-0.078855</td>\n",
       "      <td>-0.042991</td>\n",
       "      <td>-0.096283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032937</td>\n",
       "      <td>0.075821</td>\n",
       "      <td>0.030987</td>\n",
       "      <td>-0.149850</td>\n",
       "      <td>-0.003155</td>\n",
       "      <td>-0.010207</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>-0.017069</td>\n",
       "      <td>0.048123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.196893</td>\n",
       "      <td>0.113314</td>\n",
       "      <td>0.352175</td>\n",
       "      <td>-0.108499</td>\n",
       "      <td>-0.064472</td>\n",
       "      <td>-0.073239</td>\n",
       "      <td>-0.086402</td>\n",
       "      <td>0.008671</td>\n",
       "      <td>-0.342217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255167</td>\n",
       "      <td>0.096579</td>\n",
       "      <td>0.069413</td>\n",
       "      <td>-0.215386</td>\n",
       "      <td>-0.075168</td>\n",
       "      <td>-0.035071</td>\n",
       "      <td>-0.023375</td>\n",
       "      <td>0.067768</td>\n",
       "      <td>-0.181530</td>\n",
       "      <td>0.174444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.033004</td>\n",
       "      <td>0.013373</td>\n",
       "      <td>0.124001</td>\n",
       "      <td>-0.016143</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>-0.055789</td>\n",
       "      <td>-0.036282</td>\n",
       "      <td>-0.059422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035814</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>0.027321</td>\n",
       "      <td>-0.116009</td>\n",
       "      <td>0.010096</td>\n",
       "      <td>-0.042293</td>\n",
       "      <td>0.005347</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>-0.007731</td>\n",
       "      <td>0.058799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   1   0.124623   0.196628   0.257004  -0.156045  -0.054916   0.006071   \n",
       "1   2   0.109655   0.170158   0.227644  -0.127088  -0.044476  -0.046852   \n",
       "2   3   0.014854   0.030051   0.115092  -0.017179   0.002720  -0.011692   \n",
       "3   4   0.196893   0.113314   0.352175  -0.108499  -0.064472  -0.073239   \n",
       "4   5   0.033004   0.013373   0.124001  -0.016143   0.010120   0.010635   \n",
       "\n",
       "   feature_7  feature_8  feature_9  ...  feature_759  feature_760  \\\n",
       "0  -0.035149  -0.092019  -0.196302  ...    -0.221466     0.140292   \n",
       "1  -0.090026  -0.061321  -0.227288  ...    -0.204930     0.110203   \n",
       "2  -0.078855  -0.042991  -0.096283  ...    -0.032937     0.075821   \n",
       "3  -0.086402   0.008671  -0.342217  ...    -0.255167     0.096579   \n",
       "4  -0.055789  -0.036282  -0.059422  ...    -0.035814     0.093764   \n",
       "\n",
       "   feature_761  feature_762  feature_763  feature_764  feature_765  \\\n",
       "0     0.123622    -0.175572    -0.107030    -0.087621    -0.026501   \n",
       "1     0.085665    -0.286787    -0.113195    -0.057312    -0.055680   \n",
       "2     0.030987    -0.149850    -0.003155    -0.010207    -0.001427   \n",
       "3     0.069413    -0.215386    -0.075168    -0.035071    -0.023375   \n",
       "4     0.027321    -0.116009     0.010096    -0.042293     0.005347   \n",
       "\n",
       "   feature_766  feature_767  feature_768  \n",
       "0     0.139337    -0.083030     0.059507  \n",
       "1     0.143939    -0.045760     0.106113  \n",
       "2     0.000934    -0.017069     0.048123  \n",
       "3     0.067768    -0.181530     0.174444  \n",
       "4     0.007722    -0.007731     0.058799  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = separate_dataset(train, \"label_3\")\n",
    "X_valid, y_valid = separate_dataset(validation, \"label_3\")\n",
    "# X_test, y_test = separate_dataset(test, \"label_2\")\n",
    "test_md = test.drop(\"ID\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 768\n",
      "Reduced number of features: 203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA that will retain 99% of variance\n",
    "pca = PCA(n_components=0.98, whiten=True)\n",
    "\n",
    "# Fit PCA on the training set\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Apply the transformation to the validation and test sets\n",
    "X_valid_pca = pca.transform(X_valid)\n",
    "X_test_pca = pca.transform(test_md)\n",
    "\n",
    "print('Original number of features:', X_train.shape[1])\n",
    "print('Reduced number of features:', X_train_pca.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_features(X_train, X_test, X_val):\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Save the column names\n",
    "    columns = X_train.columns\n",
    "    \n",
    "    # Fit the scaler to the training data and transform\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Transform the test and validation data\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Convert the scaled features into DataFrames\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=columns)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=columns)\n",
    "    X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=columns)\n",
    "    \n",
    "    return X_train_scaled_df, X_test_scaled_df, X_val_scaled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train_pca)\n",
    "X_test_df = pd.DataFrame(X_test_pca)\n",
    "X_valid_df = pd.DataFrame(X_valid_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, X_valid_scaled = scale_features(X_train_df, X_test_df, X_valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(X_train_scaled, y_train, degree=3, C=1.0, gamma=\"auto\", kernel=\"rbf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.9978611500701262\n",
      "Testing score: 0.9933333333333333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       142\n",
      "           1       0.99      1.00      1.00       608\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       1.00      0.98      0.99       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n",
      "Confusion Matrix:\n",
      "[[137   5]\n",
      " [  0 608]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, X_train_scaled, X_valid_scaled, y_train, y_valid)\n",
    "evaluate_model_detailed(model, X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 6\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 8\n",
      "min_resources_: 12\n",
      "max_resources_: 28520\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 250\n",
      "n_resources: 12\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 84\n",
      "n_resources: 36\n",
      "Fitting 3 folds for each of 84 candidates, totalling 252 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 28\n",
      "n_resources: 108\n",
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 10\n",
      "n_resources: 324\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 4\n",
      "n_resources: 972\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 2\n",
      "n_resources: 2916\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "{'kernel': 'sigmoid', 'gamma': 0.0001, 'C': 100.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv  # explicitly require this experimental feature\n",
    "from sklearn.model_selection import HalvingRandomSearchCV  # noqa\n",
    "\n",
    "param_dist = {\n",
    "    'C': np.logspace(-4, 3, 8),  # expanded range\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    #'degree': [2, 3, 4, 5],  # added degree for 'poly' kernel\n",
    "    'gamma': ['scale', 'auto'] + list(np.logspace(-4, 3, 8))  # expanded range\n",
    "}\n",
    "\n",
    "# Assuming X_train and y_train are your training data and labels\n",
    "halving_random_search = HalvingRandomSearchCV(svm.SVC(random_state=42), param_dist, cv=3, verbose=10, n_jobs=-1, n_candidates=250)\n",
    "halving_random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(halving_random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 6\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 8\n",
      "min_resources_: 12\n",
      "max_resources_: 28520\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 250\n",
      "n_resources: 12\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 84\n",
      "n_resources: 36\n",
      "Fitting 3 folds for each of 84 candidates, totalling 252 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 28\n",
      "n_resources: 108\n",
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 10\n",
      "n_resources: 324\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 4\n",
      "n_resources: 972\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 2\n",
      "n_resources: 2916\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "{'kernel': 'poly', 'gamma': 'scale', 'degree': 4, 'C': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'C': np.logspace(-4, 3, 8),  # expanded range\n",
    "    'kernel': [ 'poly' ],\n",
    "    'degree': [2, 3, 4, 5],  # added degree for 'poly' kernel\n",
    "    'gamma': ['scale', 'auto'] + list(np.logspace(-4, 3, 8))  # expanded range\n",
    "}\n",
    "\n",
    "# Assuming X_train and y_train are your training data and labels\n",
    "halving_random_search = HalvingRandomSearchCV(svm.SVC(random_state=42), param_dist, cv=3, verbose=10, n_jobs=-1, n_candidates=250)\n",
    "halving_random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(halving_random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned = train_model(X_train_scaled, y_train, kernel=\"sigmoid\", gamma=0.0001, degree=3, C=100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.9908134642356241\n",
      "Testing score: 0.9933333333333333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       142\n",
      "           1       1.00      1.00      1.00       608\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       0.99      0.99      0.99       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n",
      "Confusion Matrix:\n",
      "[[140   2]\n",
      " [  3 605]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_tuned, X_train_scaled, X_valid_scaled, y_train, y_valid)\n",
    "evaluate_model_detailed(model_tuned, X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned1 = train_model(X_train_scaled, y_train, kernel=\"poly\", gamma=\"scale\", degree=4, C=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.7995792426367462\n",
      "Testing score: 0.8106666666666666\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       142\n",
      "           1       0.81      1.00      0.90       608\n",
      "\n",
      "    accuracy                           0.81       750\n",
      "   macro avg       0.41      0.50      0.45       750\n",
      "weighted avg       0.66      0.81      0.73       750\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0 142]\n",
      " [  0 608]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python 3.10.11\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python 3.10.11\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Python 3.10.11\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_tuned1, X_train_scaled, X_valid_scaled, y_train, y_valid)\n",
    "evaluate_model_detailed(model_tuned1, X_valid_scaled, y_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
