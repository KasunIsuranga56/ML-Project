{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "validation = pd.read_csv(\"../data/valid.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label_2'].fillna(train['label_2'].mean(), inplace=True)\n",
    "validation['label_2'].fillna(validation['label_2'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label_2'] = train['label_2'].astype(int)\n",
    "validation['label_2'] = validation['label_2'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def remove_outliers(df, threshold):\n",
    "    # Copy the DataFrame\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Remove the columns to exclude from the copy\n",
    "    df_copy = df_copy.drop([\"label_1\", \"label_2\", \"label_3\", \"label_4\"], axis=1)\n",
    "    \n",
    "    # Calculate Z-scores only on remaining columns\n",
    "    z_scores = stats.zscore(df_copy)\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    \n",
    "    # Identify outliers\n",
    "    filtered_entries = (abs_z_scores < threshold).all(axis=1)\n",
    "    \n",
    "    # Apply the mask to the original DataFrame\n",
    "    new_df = df[filtered_entries]\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# Usage:\n",
    "# exclude_cols is a list of column names to exclude\n",
    "# train_df = remove_outliers(train_df, 3, exclude_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_dataset(df, label):\n",
    "    X = df.drop([\"label_1\", \"label_2\", \"label_3\", \"label_4\"], axis=1)\n",
    "    y = df[label]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def train_model(X_train, y_train, C=1.0, kernel='rbf', degree=3, gamma='scale'):\n",
    "    model = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    print(f\"Training score: {model.score(X_train, y_train)}\")\n",
    "    print(f\"Testing score: {model.score(X_test, y_test)}\")\n",
    "\n",
    "def evaluate_model_detailed(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_759</th>\n",
       "      <th>feature_760</th>\n",
       "      <th>feature_761</th>\n",
       "      <th>feature_762</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.124623</td>\n",
       "      <td>0.196628</td>\n",
       "      <td>0.257004</td>\n",
       "      <td>-0.156045</td>\n",
       "      <td>-0.054916</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>-0.035149</td>\n",
       "      <td>-0.092019</td>\n",
       "      <td>-0.196302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221466</td>\n",
       "      <td>0.140292</td>\n",
       "      <td>0.123622</td>\n",
       "      <td>-0.175572</td>\n",
       "      <td>-0.107030</td>\n",
       "      <td>-0.087621</td>\n",
       "      <td>-0.026501</td>\n",
       "      <td>0.139337</td>\n",
       "      <td>-0.083030</td>\n",
       "      <td>0.059507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.109655</td>\n",
       "      <td>0.170158</td>\n",
       "      <td>0.227644</td>\n",
       "      <td>-0.127088</td>\n",
       "      <td>-0.044476</td>\n",
       "      <td>-0.046852</td>\n",
       "      <td>-0.090026</td>\n",
       "      <td>-0.061321</td>\n",
       "      <td>-0.227288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204930</td>\n",
       "      <td>0.110203</td>\n",
       "      <td>0.085665</td>\n",
       "      <td>-0.286787</td>\n",
       "      <td>-0.113195</td>\n",
       "      <td>-0.057312</td>\n",
       "      <td>-0.055680</td>\n",
       "      <td>0.143939</td>\n",
       "      <td>-0.045760</td>\n",
       "      <td>0.106113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.014854</td>\n",
       "      <td>0.030051</td>\n",
       "      <td>0.115092</td>\n",
       "      <td>-0.017179</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>-0.011692</td>\n",
       "      <td>-0.078855</td>\n",
       "      <td>-0.042991</td>\n",
       "      <td>-0.096283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032937</td>\n",
       "      <td>0.075821</td>\n",
       "      <td>0.030987</td>\n",
       "      <td>-0.149850</td>\n",
       "      <td>-0.003155</td>\n",
       "      <td>-0.010207</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>-0.017069</td>\n",
       "      <td>0.048123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.196893</td>\n",
       "      <td>0.113314</td>\n",
       "      <td>0.352175</td>\n",
       "      <td>-0.108499</td>\n",
       "      <td>-0.064472</td>\n",
       "      <td>-0.073239</td>\n",
       "      <td>-0.086402</td>\n",
       "      <td>0.008671</td>\n",
       "      <td>-0.342217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255167</td>\n",
       "      <td>0.096579</td>\n",
       "      <td>0.069413</td>\n",
       "      <td>-0.215386</td>\n",
       "      <td>-0.075168</td>\n",
       "      <td>-0.035071</td>\n",
       "      <td>-0.023375</td>\n",
       "      <td>0.067768</td>\n",
       "      <td>-0.181530</td>\n",
       "      <td>0.174444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.033004</td>\n",
       "      <td>0.013373</td>\n",
       "      <td>0.124001</td>\n",
       "      <td>-0.016143</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>-0.055789</td>\n",
       "      <td>-0.036282</td>\n",
       "      <td>-0.059422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035814</td>\n",
       "      <td>0.093764</td>\n",
       "      <td>0.027321</td>\n",
       "      <td>-0.116009</td>\n",
       "      <td>0.010096</td>\n",
       "      <td>-0.042293</td>\n",
       "      <td>0.005347</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>-0.007731</td>\n",
       "      <td>0.058799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   1   0.124623   0.196628   0.257004  -0.156045  -0.054916   0.006071   \n",
       "1   2   0.109655   0.170158   0.227644  -0.127088  -0.044476  -0.046852   \n",
       "2   3   0.014854   0.030051   0.115092  -0.017179   0.002720  -0.011692   \n",
       "3   4   0.196893   0.113314   0.352175  -0.108499  -0.064472  -0.073239   \n",
       "4   5   0.033004   0.013373   0.124001  -0.016143   0.010120   0.010635   \n",
       "\n",
       "   feature_7  feature_8  feature_9  ...  feature_759  feature_760  \\\n",
       "0  -0.035149  -0.092019  -0.196302  ...    -0.221466     0.140292   \n",
       "1  -0.090026  -0.061321  -0.227288  ...    -0.204930     0.110203   \n",
       "2  -0.078855  -0.042991  -0.096283  ...    -0.032937     0.075821   \n",
       "3  -0.086402   0.008671  -0.342217  ...    -0.255167     0.096579   \n",
       "4  -0.055789  -0.036282  -0.059422  ...    -0.035814     0.093764   \n",
       "\n",
       "   feature_761  feature_762  feature_763  feature_764  feature_765  \\\n",
       "0     0.123622    -0.175572    -0.107030    -0.087621    -0.026501   \n",
       "1     0.085665    -0.286787    -0.113195    -0.057312    -0.055680   \n",
       "2     0.030987    -0.149850    -0.003155    -0.010207    -0.001427   \n",
       "3     0.069413    -0.215386    -0.075168    -0.035071    -0.023375   \n",
       "4     0.027321    -0.116009     0.010096    -0.042293     0.005347   \n",
       "\n",
       "   feature_766  feature_767  feature_768  \n",
       "0     0.139337    -0.083030     0.059507  \n",
       "1     0.143939    -0.045760     0.106113  \n",
       "2     0.000934    -0.017069     0.048123  \n",
       "3     0.067768    -0.181530     0.174444  \n",
       "4     0.007722    -0.007731     0.058799  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = separate_dataset(train, \"label_4\")\n",
    "X_valid, y_valid = separate_dataset(validation, \"label_4\")\n",
    "# X_test, y_test = separate_dataset(test, \"label_2\")\n",
    "test_md = test.drop(\"ID\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 768\n",
      "Reduced number of features: 203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA that will retain 99% of variance\n",
    "pca = PCA(n_components=0.98, whiten=True)\n",
    "\n",
    "# Fit PCA on the training set\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Apply the transformation to the validation and test sets\n",
    "X_valid_pca = pca.transform(X_valid)\n",
    "X_test_pca = pca.transform(test_md)\n",
    "\n",
    "print('Original number of features:', X_train.shape[1])\n",
    "print('Reduced number of features:', X_train_pca.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_features(X_train, X_test, X_val):\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Save the column names\n",
    "    columns = X_train.columns\n",
    "    \n",
    "    # Fit the scaler to the training data and transform\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Transform the test and validation data\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Convert the scaled features into DataFrames\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=columns)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=columns)\n",
    "    X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=columns)\n",
    "    \n",
    "    return X_train_scaled_df, X_test_scaled_df, X_val_scaled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train_pca)\n",
    "X_test_df = pd.DataFrame(X_test_pca)\n",
    "X_valid_df = pd.DataFrame(X_valid_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, X_valid_scaled = scale_features(X_train_df, X_test_df, X_valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(X_train_scaled, y_train, degree=3, C=1.0, gamma=\"auto\", kernel=\"rbf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.9649018232819074\n",
      "Testing score: 0.9253333333333333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93        21\n",
      "           1       1.00      0.82      0.90        11\n",
      "           2       1.00      0.70      0.83        27\n",
      "           3       1.00      0.75      0.86         8\n",
      "           4       1.00      0.47      0.64        15\n",
      "           5       1.00      0.91      0.95        11\n",
      "           6       0.91      1.00      0.95       532\n",
      "           7       1.00      0.78      0.88        32\n",
      "           8       1.00      0.58      0.73        19\n",
      "           9       1.00      0.71      0.83        17\n",
      "          10       1.00      0.90      0.95        10\n",
      "          11       1.00      0.73      0.84        11\n",
      "          12       1.00      0.73      0.84        26\n",
      "          13       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.93       750\n",
      "   macro avg       0.99      0.77      0.86       750\n",
      "weighted avg       0.93      0.93      0.92       750\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 19   0   0   0   0   0   2   0   0   0   0   0   0   0]\n",
      " [  0   9   0   0   0   0   2   0   0   0   0   0   0   0]\n",
      " [  1   0  19   0   0   0   7   0   0   0   0   0   0   0]\n",
      " [  0   0   0   6   0   0   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   7   0   8   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  10   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0 532   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   7  25   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   8   0  11   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   5   0   0  12   0   0   0   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0   9   0   0   0]\n",
      " [  0   0   0   0   0   0   3   0   0   0   0   8   0   0]\n",
      " [  0   0   0   0   0   0   7   0   0   0   0   0  19   0]\n",
      " [  0   0   0   0   0   0   2   0   0   0   0   0   0   8]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, X_train_scaled, X_valid_scaled, y_train, y_valid)\n",
    "evaluate_model_detailed(model, X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 6\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 84\n",
      "max_resources_: 28520\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 250\n",
      "n_resources: 84\n",
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 84\n",
      "n_resources: 252\n",
      "Fitting 3 folds for each of 84 candidates, totalling 252 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 28\n",
      "n_resources: 756\n",
      "Fitting 3 folds for each of 28 candidates, totalling 84 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 10\n",
      "n_resources: 2268\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 4\n",
      "n_resources: 6804\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 2\n",
      "n_resources: 20412\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv  # explicitly require this experimental feature\n",
    "from sklearn.model_selection import HalvingRandomSearchCV  # noqa\n",
    "\n",
    "param_dist = {\n",
    "    'C': np.logspace(-4, 3, 8),  # expanded range\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    #'degree': [2, 3, 4, 5],  # added degree for 'poly' kernel\n",
    "    'gamma': ['scale', 'auto'] + list(np.logspace(-4, 3, 8))  # expanded range\n",
    "}\n",
    "\n",
    "# Assuming X_train and y_train are your training data and labels\n",
    "halving_random_search = HalvingRandomSearchCV(svm.SVC(random_state=42), param_dist, cv=3, verbose=10, n_jobs=-1, n_candidates=250)\n",
    "halving_random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(halving_random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 4\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 360\n",
      "max_resources_: 28456\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 320\n",
      "n_resources: 360\n",
      "Fitting 3 folds for each of 320 candidates, totalling 960 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 107\n",
      "n_resources: 1080\n",
      "Fitting 3 folds for each of 107 candidates, totalling 321 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 36\n",
      "n_resources: 3240\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 12\n",
      "n_resources: 9720\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "{'kernel': 'poly', 'gamma': 0.01, 'degree': 2, 'C': 10.0}\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'C': np.logspace(-4, 3, 8),  # expanded range\n",
    "    'kernel': [ 'poly' ],\n",
    "    'degree': [2, 3, 4, 5],  # added degree for 'poly' kernel\n",
    "    'gamma': ['scale', 'auto'] + list(np.logspace(-4, 3, 8))  # expanded range\n",
    "}\n",
    "\n",
    "# Assuming X_train and y_train are your training data and labels\n",
    "halving_random_search = HalvingRandomSearchCV(svm.SVC(random_state=42), param_dist, cv=3, verbose=10, n_jobs=-1, n_candidates=250)\n",
    "halving_random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(halving_random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned = train_model(X_train_scaled, y_train, kernel=\"rbf\", gamma=\"auto\", degree=4, C=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.9984186111892044\n",
      "Testing score: 0.9344919786096256\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      1.00      0.87        13\n",
      "           2       0.80      0.89      0.84         9\n",
      "           3       0.92      1.00      0.96        12\n",
      "           4       1.00      0.81      0.90        16\n",
      "           5       1.00      0.83      0.91        18\n",
      "           6       1.00      0.89      0.94         9\n",
      "           7       0.94      1.00      0.97        16\n",
      "           8       0.86      0.86      0.86        14\n",
      "           9       1.00      0.91      0.95        11\n",
      "          10       0.89      1.00      0.94         8\n",
      "          11       1.00      0.95      0.97        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       1.00      0.91      0.95        11\n",
      "          14       1.00      0.87      0.93        15\n",
      "          15       1.00      0.88      0.93        16\n",
      "          16       1.00      0.93      0.96        14\n",
      "          17       1.00      1.00      1.00        14\n",
      "          18       0.90      0.95      0.92        19\n",
      "          19       0.75      0.80      0.77        15\n",
      "          20       0.90      0.82      0.86        11\n",
      "          21       0.93      1.00      0.97        14\n",
      "          22       1.00      0.91      0.95        11\n",
      "          23       0.77      1.00      0.87        10\n",
      "          24       0.86      1.00      0.92        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      0.86      0.92         7\n",
      "          27       0.93      0.81      0.87        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.80      0.89        15\n",
      "          30       0.88      0.88      0.88         8\n",
      "          31       0.80      1.00      0.89        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.75      1.00      0.86         6\n",
      "          34       1.00      0.85      0.92        13\n",
      "          35       1.00      1.00      1.00         8\n",
      "          36       0.95      1.00      0.97        18\n",
      "          37       0.93      0.88      0.90        16\n",
      "          38       0.91      0.91      0.91        11\n",
      "          39       1.00      1.00      1.00        13\n",
      "          40       0.85      0.92      0.88        12\n",
      "          41       0.83      1.00      0.91        10\n",
      "          42       0.86      1.00      0.92        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       0.95      0.95      0.95        19\n",
      "          45       0.93      0.93      0.93        14\n",
      "          46       1.00      1.00      1.00        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       0.88      0.88      0.88        17\n",
      "          49       0.93      1.00      0.96        13\n",
      "          50       1.00      1.00      1.00        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      1.00      1.00        11\n",
      "          53       1.00      0.93      0.97        15\n",
      "          54       1.00      0.78      0.88         9\n",
      "          55       0.89      1.00      0.94         8\n",
      "          56       0.90      0.90      0.90        10\n",
      "          57       1.00      0.94      0.97        18\n",
      "          58       0.95      0.95      0.95        20\n",
      "          59       0.91      1.00      0.95        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.93       748\n",
      "   macro avg       0.94      0.94      0.94       748\n",
      "weighted avg       0.94      0.93      0.93       748\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13  0  0 ...  0  0  0]\n",
      " [ 0  8  1 ...  0  0  0]\n",
      " [ 0  0 12 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 19  1  0]\n",
      " [ 0  0  0 ...  0 10  0]\n",
      " [ 0  0  0 ...  0  0 10]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_tuned, X_train_scaled, X_valid_scaled, y_train, y_valid)\n",
    "evaluate_model_detailed(model_tuned, X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned1 = train_model(X_train_scaled, y_train, kernel=\"poly\", gamma=0.01, degree=2, C=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.9993323025021085\n",
      "Testing score: 0.9224598930481284\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.92      0.83        13\n",
      "           2       0.90      1.00      0.95         9\n",
      "           3       0.92      0.92      0.92        12\n",
      "           4       1.00      0.88      0.93        16\n",
      "           5       0.88      0.78      0.82        18\n",
      "           6       0.89      0.89      0.89         9\n",
      "           7       0.76      1.00      0.86        16\n",
      "           8       0.67      0.86      0.75        14\n",
      "           9       0.91      0.91      0.91        11\n",
      "          10       0.80      1.00      0.89         8\n",
      "          11       0.94      0.89      0.92        19\n",
      "          12       1.00      1.00      1.00         7\n",
      "          13       0.90      0.82      0.86        11\n",
      "          14       0.93      0.93      0.93        15\n",
      "          15       0.93      0.88      0.90        16\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.92      0.86      0.89        14\n",
      "          18       0.88      0.79      0.83        19\n",
      "          19       0.81      0.87      0.84        15\n",
      "          20       0.79      1.00      0.88        11\n",
      "          21       1.00      1.00      1.00        14\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       0.64      0.90      0.75        10\n",
      "          24       0.92      1.00      0.96        12\n",
      "          25       1.00      1.00      1.00        11\n",
      "          26       1.00      1.00      1.00         7\n",
      "          27       1.00      0.88      0.93        16\n",
      "          28       1.00      1.00      1.00        11\n",
      "          29       1.00      0.73      0.85        15\n",
      "          30       1.00      0.88      0.93         8\n",
      "          31       1.00      0.92      0.96        12\n",
      "          32       1.00      1.00      1.00         9\n",
      "          33       0.67      1.00      0.80         6\n",
      "          34       1.00      0.77      0.87        13\n",
      "          35       0.89      1.00      0.94         8\n",
      "          36       0.94      0.94      0.94        18\n",
      "          37       0.93      0.81      0.87        16\n",
      "          38       1.00      0.73      0.84        11\n",
      "          39       0.92      0.92      0.92        13\n",
      "          40       0.92      0.92      0.92        12\n",
      "          41       0.91      1.00      0.95        10\n",
      "          42       0.92      1.00      0.96        12\n",
      "          43       1.00      1.00      1.00        12\n",
      "          44       1.00      0.89      0.94        19\n",
      "          45       1.00      0.93      0.96        14\n",
      "          46       0.85      1.00      0.92        11\n",
      "          47       1.00      1.00      1.00         8\n",
      "          48       0.89      1.00      0.94        17\n",
      "          49       1.00      1.00      1.00        13\n",
      "          50       1.00      0.92      0.96        13\n",
      "          51       1.00      1.00      1.00         8\n",
      "          52       1.00      0.91      0.95        11\n",
      "          53       0.94      1.00      0.97        15\n",
      "          54       1.00      0.89      0.94         9\n",
      "          55       1.00      1.00      1.00         8\n",
      "          56       1.00      0.90      0.95        10\n",
      "          57       1.00      1.00      1.00        18\n",
      "          58       1.00      0.95      0.97        20\n",
      "          59       0.89      0.80      0.84        10\n",
      "          60       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.92       748\n",
      "   macro avg       0.93      0.93      0.93       748\n",
      "weighted avg       0.93      0.92      0.92       748\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12  0  0 ...  0  0  0]\n",
      " [ 0  9  0 ...  0  0  0]\n",
      " [ 0  0 11 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 19  0  0]\n",
      " [ 0  0  0 ...  0  8  0]\n",
      " [ 0  0  0 ...  0  0 10]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_tuned1, X_train_scaled, X_valid_scaled, y_train, y_valid)\n",
    "evaluate_model_detailed(model_tuned1, X_valid_scaled, y_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
